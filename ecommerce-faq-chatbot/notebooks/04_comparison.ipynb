{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Evaluation: Zero-Shot vs Fine-Tuned vs RAG\n",
    "\n",
    "This notebook compares three approaches for e-commerce FAQ response generation:\n",
    "1. **Zero-shot**: Base Falcon-7B without fine-tuning\n",
    "2. **Fine-tuned**: Falcon-7B with LoRA adapters\n",
    "3. **RAG**: Retrieval-Augmented Generation with vector similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets peft bitsandbytes accelerate nltk pandas matplotlib seaborn faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tiiuae/falcon-7b\"\n",
    "ADAPTER_PATH = \"./falcon-7b-ecommerce-lora\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "full_data = dataset['train']\n",
    "test_data = full_data.shuffle(seed=42).select(range(50))\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "rag_data = full_data.select(range(min(5000, len(full_data))))\n",
    "instructions = [item['instruction'] for item in rag_data]\n",
    "responses = [item['response'] for item in rag_data]\n",
    "\n",
    "print(\"Creating embeddings...\")\n",
    "embeddings = embedding_model.encode(instructions, show_progress_bar=True)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index created with {index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar(query, k=3):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(np.array(query_embedding).astype('float32'), k)\n",
    "    \n",
    "    retrieved = []\n",
    "    for idx in indices[0]:\n",
    "        retrieved.append({\n",
    "            'instruction': instructions[idx],\n",
    "            'response': responses[idx]\n",
    "        })\n",
    "    return retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zero_shot(query):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are a helpful e-commerce customer support assistant. Answer the customer's question professionally and helpfully.\n",
    "\n",
    "### Customer Query:\n",
    "{query}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_finetuned(query):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are a helpful e-commerce customer support assistant. Answer the customer's question professionally and helpfully.\n",
    "\n",
    "### Customer Query:\n",
    "{query}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(finetuned_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = finetuned_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag(query):\n",
    "    retrieved = retrieve_similar(query, k=3)\n",
    "    \n",
    "    context = \"\"\n",
    "    for i, item in enumerate(retrieved, 1):\n",
    "        context += f\"Example {i}:\\nQ: {item['instruction']}\\nA: {item['response']}\\n\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are a helpful e-commerce customer support assistant. Use the following examples to help answer the customer's question.\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Customer Query:\n",
    "{query}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(base_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Comparative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, generated):\n",
    "    ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "    gen_tokens = nltk.word_tokenize(generated.lower())\n",
    "    \n",
    "    smoothie = SmoothingFunction().method1\n",
    "    \n",
    "    bleu1 = sentence_bleu([ref_tokens], gen_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu4 = sentence_bleu([ref_tokens], gen_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "    \n",
    "    return bleu1, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for sample in tqdm(test_data, desc=\"Evaluating\"):\n",
    "    query = sample['instruction']\n",
    "    reference = sample['response']\n",
    "    \n",
    "    zero_shot_resp = generate_zero_shot(query)\n",
    "    finetuned_resp = generate_finetuned(query)\n",
    "    rag_resp = generate_rag(query)\n",
    "    \n",
    "    zs_b1, zs_b4 = calculate_bleu(reference, zero_shot_resp)\n",
    "    ft_b1, ft_b4 = calculate_bleu(reference, finetuned_resp)\n",
    "    rag_b1, rag_b4 = calculate_bleu(reference, rag_resp)\n",
    "    \n",
    "    results.append({\n",
    "        'query': query,\n",
    "        'reference': reference,\n",
    "        'zero_shot_response': zero_shot_resp,\n",
    "        'finetuned_response': finetuned_resp,\n",
    "        'rag_response': rag_resp,\n",
    "        'zero_shot_bleu1': zs_b1,\n",
    "        'zero_shot_bleu4': zs_b4,\n",
    "        'finetuned_bleu1': ft_b1,\n",
    "        'finetuned_bleu4': ft_b4,\n",
    "        'rag_bleu1': rag_b1,\n",
    "        'rag_bleu4': rag_b4\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('comparison_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame({\n",
    "    'Approach': ['Zero-Shot', 'Fine-Tuned', 'RAG'],\n",
    "    'BLEU-1 Mean': [\n",
    "        results_df['zero_shot_bleu1'].mean(),\n",
    "        results_df['finetuned_bleu1'].mean(),\n",
    "        results_df['rag_bleu1'].mean()\n",
    "    ],\n",
    "    'BLEU-1 Std': [\n",
    "        results_df['zero_shot_bleu1'].std(),\n",
    "        results_df['finetuned_bleu1'].std(),\n",
    "        results_df['rag_bleu1'].std()\n",
    "    ],\n",
    "    'BLEU-4 Mean': [\n",
    "        results_df['zero_shot_bleu4'].mean(),\n",
    "        results_df['finetuned_bleu4'].mean(),\n",
    "        results_df['rag_bleu4'].mean()\n",
    "    ],\n",
    "    'BLEU-4 Std': [\n",
    "        results_df['zero_shot_bleu4'].std(),\n",
    "        results_df['finetuned_bleu4'].std(),\n",
    "        results_df['rag_bleu4'].std()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nComparative Results Summary:\")\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x, summary['BLEU-1 Mean'], width, yerr=summary['BLEU-1 Std'], capsize=5)\n",
    "axes[0].set_ylabel('BLEU-1 Score')\n",
    "axes[0].set_title('BLEU-1 Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(summary['Approach'])\n",
    "\n",
    "axes[1].bar(x, summary['BLEU-4 Mean'], width, yerr=summary['BLEU-4 Std'], capsize=5, color='orange')\n",
    "axes[1].set_ylabel('BLEU-4 Score')\n",
    "axes[1].set_title('BLEU-4 Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(summary['Approach'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_chart.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(results_df['zero_shot_bleu4'], bins=15, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Zero-Shot BLEU-4')\n",
    "axes[0].set_xlabel('Score')\n",
    "\n",
    "axes[1].hist(results_df['finetuned_bleu4'], bins=15, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[1].set_title('Fine-Tuned BLEU-4')\n",
    "axes[1].set_xlabel('Score')\n",
    "\n",
    "axes[2].hist(results_df['rag_bleu4'], bins=15, alpha=0.7, edgecolor='black', color='green')\n",
    "axes[2].set_title('RAG BLEU-4')\n",
    "axes[2].set_xlabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('score_distributions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Qualitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"SAMPLE COMPARISONS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i in range(min(5, len(results_df))):\n",
    "    row = results_df.iloc[i]\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Query: {row['query'][:100]}...\")\n",
    "    print(f\"\\nReference: {row['reference'][:200]}...\")\n",
    "    print(f\"\\nZero-Shot (BLEU-4: {row['zero_shot_bleu4']:.3f}): {row['zero_shot_response'][:200]}...\")\n",
    "    print(f\"\\nFine-Tuned (BLEU-4: {row['finetuned_bleu4']:.3f}): {row['finetuned_response'][:200]}...\")\n",
    "    print(f\"\\nRAG (BLEU-4: {row['rag_bleu4']:.3f}): {row['rag_response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Win Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_winner(row):\n",
    "    scores = {\n",
    "        'Zero-Shot': row['zero_shot_bleu4'],\n",
    "        'Fine-Tuned': row['finetuned_bleu4'],\n",
    "        'RAG': row['rag_bleu4']\n",
    "    }\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "results_df['winner'] = results_df.apply(get_winner, axis=1)\n",
    "win_counts = results_df['winner'].value_counts()\n",
    "\n",
    "print(\"\\nWin Rate Analysis (based on BLEU-4):\")\n",
    "for approach, count in win_counts.items():\n",
    "    print(f\"{approach}: {count} wins ({count/len(results_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "plt.pie(win_counts.values, labels=win_counts.index, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "plt.title('Win Rate Distribution (BLEU-4)')\n",
    "plt.savefig('win_rate.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv('comparison_summary.csv', index=False)\n",
    "print(\"Results saved to comparison_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### 1. Fine-Tuning Advantages\n",
    "- Best performance on domain-specific queries\n",
    "- Learns e-commerce terminology and response patterns\n",
    "- Consistent response quality\n",
    "\n",
    "### 2. Zero-Shot Limitations\n",
    "- Generic responses lacking domain context\n",
    "- Struggles with specific e-commerce scenarios\n",
    "- Lower BLEU scores overall\n",
    "\n",
    "### 3. RAG Trade-offs\n",
    "- Good for factual queries with similar examples\n",
    "- Performance depends on retrieval quality\n",
    "- No training required, faster to deploy\n",
    "\n",
    "### 4. Recommendations\n",
    "- Use fine-tuning for production customer support\n",
    "- RAG is suitable for knowledge-intensive queries\n",
    "- Consider hybrid approaches for best results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
