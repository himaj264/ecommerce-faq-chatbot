{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook evaluates the fine-tuned Falcon-7B model using BLEU scores and qualitative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers datasets peft bitsandbytes accelerate nltk pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"tiiuae/falcon-7b\"\n",
    "ADAPTER_PATH = \"./falcon-7b-ecommerce-lora\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "test_data = dataset['train'].shuffle(seed=42).select(range(100))\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "You are a helpful e-commerce customer support assistant. Answer the customer's question professionally and helpfully.\n",
    "\n",
    "### Customer Query:\n",
    "{query}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"### Response:\")[-1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for sample in tqdm(test_data, desc=\"Generating responses\"):\n",
    "    generated = generate_response(sample['instruction'])\n",
    "    results.append({\n",
    "        'query': sample['instruction'],\n",
    "        'reference': sample['response'],\n",
    "        'generated': generated,\n",
    "        'category': sample['category']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('evaluation_results.csv', index=False)\n",
    "print(\"Results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, generated):\n",
    "    ref_tokens = nltk.word_tokenize(reference.lower())\n",
    "    gen_tokens = nltk.word_tokenize(generated.lower())\n",
    "    \n",
    "    smoothie = SmoothingFunction().method1\n",
    "    \n",
    "    bleu1 = sentence_bleu([ref_tokens], gen_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu2 = sentence_bleu([ref_tokens], gen_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "    bleu4 = sentence_bleu([ref_tokens], gen_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "    \n",
    "    return bleu1, bleu2, bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = []\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    b1, b2, b4 = calculate_bleu(row['reference'], row['generated'])\n",
    "    bleu_scores.append({'bleu1': b1, 'bleu2': b2, 'bleu4': b4})\n",
    "\n",
    "bleu_df = pd.DataFrame(bleu_scores)\n",
    "results_df = pd.concat([results_df, bleu_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BLEU Score Summary:\")\n",
    "print(f\"BLEU-1: {results_df['bleu1'].mean():.4f} (+/- {results_df['bleu1'].std():.4f})\")\n",
    "print(f\"BLEU-2: {results_df['bleu2'].mean():.4f} (+/- {results_df['bleu2'].std():.4f})\")\n",
    "print(f\"BLEU-4: {results_df['bleu4'].mean():.4f} (+/- {results_df['bleu4'].std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(results_df['bleu1'], bins=20, edgecolor='black')\n",
    "axes[0].set_title('BLEU-1 Distribution')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(results_df['bleu2'], bins=20, edgecolor='black')\n",
    "axes[1].set_title('BLEU-2 Distribution')\n",
    "axes[1].set_xlabel('Score')\n",
    "\n",
    "axes[2].hist(results_df['bleu4'], bins=20, edgecolor='black')\n",
    "axes[2].set_title('BLEU-4 Distribution')\n",
    "axes[2].set_xlabel('Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bleu_distributions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_bleu = results_df.groupby('category')[['bleu1', 'bleu4']].mean().sort_values('bleu4', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "category_bleu.head(10).plot(kind='bar')\n",
    "plt.title('BLEU Scores by Category (Top 10)')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('BLEU Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(['BLEU-1', 'BLEU-4'])\n",
    "plt.tight_layout()\n",
    "plt.savefig('bleu_by_category.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Qualitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TOP 5 BEST RESPONSES (by BLEU-4)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "top_samples = results_df.nlargest(5, 'bleu4')\n",
    "for i, row in top_samples.iterrows():\n",
    "    print(f\"\\nQuery: {row['query'][:100]}...\")\n",
    "    print(f\"Reference: {row['reference'][:150]}...\")\n",
    "    print(f\"Generated: {row['generated'][:150]}...\")\n",
    "    print(f\"BLEU-4: {row['bleu4']:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"BOTTOM 5 RESPONSES (by BLEU-4)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "bottom_samples = results_df.nsmallest(5, 'bleu4')\n",
    "for i, row in bottom_samples.iterrows():\n",
    "    print(f\"\\nQuery: {row['query'][:100]}...\")\n",
    "    print(f\"Reference: {row['reference'][:150]}...\")\n",
    "    print(f\"Generated: {row['generated'][:150]}...\")\n",
    "    print(f\"BLEU-4: {row['bleu4']:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Response Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['ref_length'] = results_df['reference'].str.len()\n",
    "results_df['gen_length'] = results_df['generated'].str.len()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(results_df['ref_length'], results_df['gen_length'], alpha=0.5)\n",
    "plt.plot([0, 1000], [0, 1000], 'r--')\n",
    "plt.xlabel('Reference Length')\n",
    "plt.ylabel('Generated Length')\n",
    "plt.title('Response Length Comparison')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(results_df['gen_length'], results_df['bleu4'], alpha=0.5)\n",
    "plt.xlabel('Generated Response Length')\n",
    "plt.ylabel('BLEU-4 Score')\n",
    "plt.title('Length vs BLEU-4')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('length_analysis.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'metric': ['BLEU-1', 'BLEU-2', 'BLEU-4'],\n",
    "    'mean': [results_df['bleu1'].mean(), results_df['bleu2'].mean(), results_df['bleu4'].mean()],\n",
    "    'std': [results_df['bleu1'].std(), results_df['bleu2'].std(), results_df['bleu4'].std()],\n",
    "    'min': [results_df['bleu1'].min(), results_df['bleu2'].min(), results_df['bleu4'].min()],\n",
    "    'max': [results_df['bleu1'].max(), results_df['bleu2'].max(), results_df['bleu4'].max()]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.to_csv('evaluation_summary.csv', index=False)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Evaluation complete. Output files:\n",
    "- `evaluation_results.csv` - All generated responses with BLEU scores\n",
    "- `evaluation_summary.csv` - Summary statistics\n",
    "- `bleu_distributions.png` - BLEU score distributions\n",
    "- `bleu_by_category.png` - Performance by category\n",
    "- `length_analysis.png` - Response length analysis"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
